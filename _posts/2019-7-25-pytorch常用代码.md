---
title: pytorch常用代码
data: 2019-7-25
categories: pytorch
---

```python
torch.randperm(n)  #返回[0,n-1]的数组
torch.max(tensor)  # 返回tensor中的最大元素
torch.max(tensor, 0)  # 返回每列的最大元素及其索引
torch.max(tensor, 1)  # 返回每行的最大元素及其索引
torch.sum(tensor, 0)  # 按列求和
torch.sum(tensor, 1)  # 按行求和
# pytorch中为cpu和gpu设置随机数种子
torch.manual_seed(seed)
torch.cuda.manual_seed(seed)
```

```python
#pytorch学习率调整策略
torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10, verbose=False, threshold=0.0001, threshold_mode='rel', cooldown=0, min_lr=0, eps=1e-08)
# mode 表示当监控量不再下降时减小学习率
# factor 每次学习率减小的比率
# patience 容忍网络性能不提升的次数，高于此次数即减小学习率

#使用方法如下：
optimizer = torch.optim.SGD(model.parameters(), lr=0.01) 
scheduler = ReduceLROnPlateau(optimizer, 'min',factor=0.5, patience=4, verbose=True)
scheduler.step(val_loss)
```

```python
#pytorch中需要定义自己的数据类，继承于torch.utils.data.Dataset，需要实现__len__,__getitem__两个函数。
# pytorch中使用torch.utils.data.DataLoader来定义迭代器，按照batch_size将数据封装成tensor作为模型的输入。 
from torch.utils.data import Dataset, DataLoader
# pytorch中的数据预处理与增强使用torchvision.transforms
from torchvision import transforms
transforms.ToTensor()  #将PIL Image或ndarray转化为tensor，并归一化到[0-1]
transforms.Normalize()  #标准化

# pytorh参数的保存与加载
torch.save(model.state_dict(), "model.pth")
model.load_state_dict(torch.load("model.pth"))

# 对于在训练阶段和测试阶段的Batch Normalization和Dropout的不同，pytorh中使用model.train()和model.eval()进行区分。
```

```python
tensor.item()  # 若tensor只有一个元素，则调用item会将tensor转化为python中的标量，若tensor不止一个元素，则会出错。
with torch.no_grad()  # 此范围内的代码，将不会进行反向传播，降低了内存使用并加速计算
model.modules # 作为迭代器时，会遍历model中所有的子层
```



