---
title: DL-model知识点
data: 2019-7-24
categories: model
---

#### 图像分类任务中的tricks

- 学习率预热就是在刚开始训练的时候先使用一个较小的学习率，训练一些epoches或iterations，等模型稳定时再修改为预先设置的学习率进行训练。
- Mixup是一种新的数据增强的方法。Mixup training，就是每次取出2张图片，然后将它们线性组合，得到新的图片，以此来作为新的训练样本，进行网络的训练。
- 余弦学习率衰减

#### 特征工程

##### 特征归一化

###### 数值型特征

- 为了消除数据特征之间的量纲影响，使不同的数据之间有可比性。
- 对于梯度下降来说，能更快的收敛找到最优解。
- 对于决策树模型，归一化并不适用，因为决策树在分裂点主要取决于数据集关于特征的信息增益比。

###### 类别型特征

- 只有决策树可以直接处理类别型特征的字符串输出。逻辑回归、支持向量机等需要将特征转化成数值型特征才能工作。
- 将类别型特征转化为数值型特征的方法：
  - 序号编码：根据大小关系
  - 独热编码：表示成向量的形式，只有某一维为1，其余为0。在类别数较多的情况下，可使用向量的稀疏表示来节省空间。
  - 二进制编码

##### 模型评估

- 准确率：分类正确的样本占样本的比例。
- 精确率：分类正确的正样本个数占分类器判别为正样本个数的比例。
- 召回率：分类正确的正样本个数占所有真正的正样本个数的比例。
- P-R曲线：横轴为召回率，纵轴为精确率，曲线上的一点表示将大于此阈值的判定为正样本，小于此阈值判定为负样本，此时返回结果对应的精确率和召回率。
- F1-score
- ROC曲线：横轴为FPR（假阳性率），纵轴为TPR（真阳性率）。
- AUC：ROC曲线下的面积，该值能够量化的反应基于ROC曲线衡量模型的性能。
- ROC曲线相对于P-R曲线的特点是，当正负样本的分布发生变化时，ROC曲线能基本保持不变，而P-R曲线会发生很大的变化。对于样本不平衡的情况下，ROC曲线更加适用，对于比较特定数据集模型的性能，P-R曲线合适。
- 模型评估的方法：K折交叉验证

##### 超参数调优

- 网格搜索
- 随机搜索
- 贝叶斯优化算法

##### 降低过拟合的方法

- 获取更多的数据
- 降低模型复杂度
- 正则化
- 集成学习方法

##### 降低欠拟合的方法

- 添加新的特征
- 增加模型复杂度
- 减小正则化程度

#### 常见知识点

##### 模型压缩需要解决的三个问题

- 模型的大小 
- 运行时的内存    
- 计算量

##### 模型压缩的方法

- 通道剪枝 
  - 通过最小化剪裁后特征图和剪裁前特征图之间的误差，尽可能减小卷积核的通道数。
- 权重稀疏化
  - 通过对网络权重引入稀疏性约束，可以大幅度降低网络权重中的非零元素个数；压缩后模型的网络权重可以以稀疏矩阵的形式进行存储和传输，从而实现模型压缩。
- 权重量化
  - 使用uint8类型代替float32类型来存储权重。
- 网络蒸馏
  - 将未压缩的模型的输出作为监督信息，知道压缩模型的训练。

##### 1×1卷积的作用

- **灵活的控制特征图的深度（升维、降维）。**
- **减少参数。**
- **实现跨通道信息组合，增加了非线性特征。**

##### relu的作用

- **减少梯度计算。大于0的部分导数为1。**
- **解决梯度消失。**
- **弱化过拟合。引入了网络稀疏性。**

##### resnext

- **结合了resnet和inception的思想。残差部分为并行的相同的分支结构**

  ![img](http://note.youdao.com/yws/res/1494/WEBRESOURCE6a2e01bba67c73347df8af27dbd06cf1)

##### fpn

- **处理目标检测中多尺度的问题。一般使用最后一层的特征，具有较大的感受野，但是对小目标的检测结果不好。**
- **通过自底向上，自顶向下的特征提取过程，将高层的语义信息和低层的高分辨率信息结合，这样有利用处理多尺度的问题。**

##### 余旋相似度和欧式距离

![img](http://note.youdao.com/yws/res/1509/WEBRESOURCEb909760e9088b5a952ffcd1734a72843)

##### l2范数计算

![img](http://note.youdao.com/yws/res/1600/WEBRESOURCE24eb4c9360c9649d93568270dae07dd1)

##### 人脸识别中的l2范数归一化

- **l2范数归一化，将特征映射到超球面。网络的重点将会放在角度空间。会消除因视角、光照、遮挡等造成的影响。**

##### 目标检测中样本不平衡的问题

- **获取更多少量样本的数据。**
- **focal loss     重塑标准交叉熵损失，降低简单负面样本所占的权重。**
- **OHEM    将候选区域的损失值进行排序，选择损失值大的一部分样本训练，使网络更关注于图像中更困难的目标。**

##### 模型评估标准

- **准确率      样本不平衡时不适用，如癌症检测的例子。**
- **精确率	真正的正例占所有被识别为正例的比例，tp / (tp + fp)**
- **召回率	真正的正例占所有正例的比例，tp / (tp + fn)**
- **ROC曲线    以fpr为横轴，tpr为纵轴，取不同的阈值所画出来的曲线，曲线下的面积叫做AUC**
- **PR曲线	以召回率为横轴，精确率为纵轴画出来的曲线**

##### Batch Normalization

- **深层网络难以训练，是因为层与层之间的关联性，在训练的过程中，参数随着梯度下降在不断的更新，参数的变化会导致每一层的输入分布发生变化，上层的网络需要不停的适应这些变化，会导致训练变慢，使得训练困难。**

- **激活函数梯度饱和   --   对于sigmoid类的激活函数，当输出太大时，会进入饱和区，梯度会很小甚至接近于0，则会导致参数更新慢，减慢了网络收敛的速度。**

- - **可使用非饱和性的激活函数，如relu。**
  - **使激活函数的输入分布保持在一个稳定的状态避免进入饱和区，如Batch Normalization。**

- **白化（Whitening）是机器学习中规范化数据的方法。**

- - **使得输入特征分布具有相同的均值和方差。PCA白化使得所有特征分布的均值为0，方差为1。**
  - **去除特征之间的相关性。**
  - **缺点：计算成本太高，改变了每一层的分布。**

- **Batch Normalization**

- - **使得L层每个特征的均值为0，方差为1。但是这样会使得经过sigmoid类的激活函数时，陷入非线性函数的线性区域。**
  - **因此引入了两个可学习的参数γ和β，这两个参数的引入是为了恢复数据本身的表达能力。对规范化后的数据进行线性变换。**

- **测试阶段的Batch Normalization**

- - **训练阶段的均值和方差都是通过mini_batch的数据计算而来，而测试时，一般只需要测试一个数据或很少的数据，对这些很少的数据计算均值和方差会产生偏差，因此测试阶段的均值和方差使用训练阶段保存的均值和方差计算无偏估计得到。**

- **Batch Normalization 优点：**

- - **加快了网络收敛的速度。    后层的网络不必不断适应底层网络输入的变化。**
  - **BN使得模型对网络中的参数不那么敏感，简化了调参的过程。如学习率。**
  - **有利于缓解梯度消失的问题。    可以使得通过sigmoid类的激活函数时落在非饱和区域。**
  - **有一定正则化的效果。     通过将mini_batch样本的均值和方差作为整体样本均值和方差的估计，虽然mini_batch的样本是从整体样本采样得到，但不同的mini_batch的均值和方差会有所不同，相当于在网络的学习过程中添加了随机噪声。**